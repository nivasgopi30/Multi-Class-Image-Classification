{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb9184fa-4fc0-4b23-8e91-2be402dd8b70",
   "metadata": {},
   "source": [
    "## Importing Modules Required (We will also import whenever required!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdfd95e3-0e27-4158-ae70-5225048388bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13535a1-c8b5-458e-b980-dff4553a712a",
   "metadata": {},
   "source": [
    "## Using Transfer Learning Technique for Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2447ec7-3bf2-4c82-9d20-5dbbd519a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using InceptionResNetV2 Convolutional Neural Network for extracting features.\n",
    "# I'm using InceptionResNetV2 because it performs better than rest of the models if you check in Keras Official Docs\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "# We are preprocessing the input to be given for better results.\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1313e3c1-d05e-465d-b8da-7126e6322b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use ImageDataGenerator for reading images\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d8358-ed55-4269-a1df-c58f6597624c",
   "metadata": {},
   "source": [
    "## Data Augmentation for Better Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aaa8f43-1237-45bc-9b89-92042ae7310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same ImageDataGenerator for augmenting images which means zooming, stretching, sliding the images.\n",
    "train_image_gen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                     rotation_range=360,\n",
    "                                     width_shift_range=0.4,\n",
    "                                    height_shift_range=0.4,\n",
    "                                    zoom_range=0.5,\n",
    "                                    horizontal_flip=True,\n",
    "                                    vertical_flip=True,\n",
    "                                    shear_range=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "851ac52e-9b27-4212-aeef-bc9e95b7c527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4750 images belonging to 12 classes.\n"
     ]
    }
   ],
   "source": [
    "# Reading input images with the help of flow_from_directory function. \n",
    "# We commonly specify the tensor size of image(literally image size) i.e target size as (299,299) which will be \n",
    "# good for training the model.\n",
    "train_images = train_image_gen.flow_from_directory('./train',batch_size=32,target_size=(299,299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caf3d56e-8fdf-466a-8fef-a6513deb0365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Black-grass': 0,\n",
       " 'Charlock': 1,\n",
       " 'Cleavers': 2,\n",
       " 'Common Chickweed': 3,\n",
       " 'Common wheat': 4,\n",
       " 'Fat Hen': 5,\n",
       " 'Loose Silky-bent': 6,\n",
       " 'Maize': 7,\n",
       " 'Scentless Mayweed': 8,\n",
       " 'Shepherds Purse': 9,\n",
       " 'Small-flowered Cranesbill': 10,\n",
       " 'Sugar beet': 11}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observing the classes of training data.\n",
    "train_images.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c978194-56cd-40b1-9b40-48f409f4a2bf",
   "metadata": {},
   "source": [
    "## Data is ready!!! Let's create a model for training the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acbd5ee1-9052-4ee2-a248-c8d13f6330c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are giving the input tensor to CNNs.\n",
    "from tensorflow.keras.layers import Input\n",
    "# Here 3 in the input shape denotes there are three channels for every image which are Red, Green and Blue.\n",
    "primary_input = Input(shape=(299,299,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f728c9-0d60-4167-a65a-8c6413d455f3",
   "metadata": {},
   "source": [
    "#### Initiating the base model and giving input to it. We don't want top dense layers of InceptionResNetV2 as it deals with another task, so we set 'include_top' parameter to False. We will give top layers separately according to our task specified - This is known as TRANSFER lEARNING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35d2c227-e9c6-430d-93a5-b8075089ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I'm using the Funtional API of Keras for dealing with layers initialization.\n",
    "base_model = InceptionResNetV2(include_top=False,input_shape=(299,299,3))(primary_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b02eea7c-403c-4581-a5eb-e5ec12839a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 8, 8, 1536])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a366f56-bdbe-4f6e-b759-b28de6bbee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the base_model tensor so as we can give the flattened array to Dense Layers.\n",
    "from tensorflow.keras.layers import Flatten\n",
    "vector_representation = Flatten()(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64885697-c476-40ed-98e4-81ce9893d09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 98304) dtype=float32 (created by layer 'flatten_1')>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how nicely it is flattened.\n",
    "vector_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835e6b5-1cad-40ba-bbc4-4c518a9b0c2e",
   "metadata": {},
   "source": [
    "## Initiating the dense layers which I mentioned before as top layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "653d9d93-beca-4eff-9e20-f50f18283e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "# Created a dense layer with 128 neurons and given the vector as input for dense layer.\n",
    "dense_layer = Dense(128, activation='relu')(vector_representation)\n",
    "# Created another layer with 12 neurons as we have 12 classes to classify an image which results as output.\n",
    "output = Dense(12, activation='softmax')(dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d68bcc4-7db9-4b03-bb03-8f59e24f04d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name=None), name='dense_1/Softmax:0', description=\"created by layer 'dense_1'\")\n"
     ]
    }
   ],
   "source": [
    "# Check the output whether it is having 12 columns\n",
    "print(output)  # Yeah we have!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8cf286-9be2-4995-82c4-b3587a29686f",
   "metadata": {},
   "source": [
    "## All layers in the neural network has been set. Now Let's create our final model for training and predicting!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78d9c40f-e491-4758-9da0-bab9dc411af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "model = Model(primary_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "649cec76-5566-4c72-a3c3-19ecc49ac788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing the learning rate with Adam Optimizer which is considerably better suited.\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# You can tune the learning rate hyperparameter as you wish.\n",
    "optimizer = Adam(0.001)  \n",
    "# Let's compile our model. Here we are specifying 'loss' as categorical_crossentropy because our task is multi-class \n",
    "# image classification.\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5a8fd-3780-4e5f-a8b1-04b5bb996f23",
   "metadata": {},
   "source": [
    "## Model set!! What are we waiting for???!!! Let's train the model by fitting the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee5dc077-450a-401e-b31e-e333dd153b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "149/149 [==============================] - 161s 1s/step - loss: 3.0053 - accuracy: 0.1145\n",
      "Epoch 2/20\n",
      "149/149 [==============================] - 153s 1s/step - loss: 2.3686 - accuracy: 0.1531\n",
      "Epoch 3/20\n",
      "149/149 [==============================] - 157s 1s/step - loss: 1.7998 - accuracy: 0.3813\n",
      "Epoch 4/20\n",
      "149/149 [==============================] - 162s 1s/step - loss: 1.3439 - accuracy: 0.5514\n",
      "Epoch 5/20\n",
      "149/149 [==============================] - 162s 1s/step - loss: 1.0709 - accuracy: 0.6518\n",
      "Epoch 6/20\n",
      "149/149 [==============================] - 162s 1s/step - loss: 0.7237 - accuracy: 0.7558\n",
      "Epoch 7/20\n",
      "149/149 [==============================] - 157s 1s/step - loss: 0.6212 - accuracy: 0.7863\n",
      "Epoch 8/20\n",
      "149/149 [==============================] - 152s 1s/step - loss: 0.5189 - accuracy: 0.8227\n",
      "Epoch 9/20\n",
      "149/149 [==============================] - 155s 1s/step - loss: 0.5055 - accuracy: 0.8246\n",
      "Epoch 10/20\n",
      "149/149 [==============================] - 162s 1s/step - loss: 0.4465 - accuracy: 0.8484\n",
      "Epoch 11/20\n",
      "149/149 [==============================] - 153s 1s/step - loss: 0.3834 - accuracy: 0.8714\n",
      "Epoch 12/20\n",
      "149/149 [==============================] - 158s 1s/step - loss: 0.3755 - accuracy: 0.8733\n",
      "Epoch 13/20\n",
      "149/149 [==============================] - 162s 1s/step - loss: 0.3635 - accuracy: 0.8815\n",
      "Epoch 14/20\n",
      "149/149 [==============================] - 157s 1s/step - loss: 0.3413 - accuracy: 0.8924\n",
      "Epoch 15/20\n",
      "149/149 [==============================] - 155s 1s/step - loss: 0.3369 - accuracy: 0.8878\n",
      "Epoch 16/20\n",
      "149/149 [==============================] - 162s 1s/step - loss: 0.3361 - accuracy: 0.8891\n",
      "Epoch 17/20\n",
      "149/149 [==============================] - 163s 1s/step - loss: 0.3040 - accuracy: 0.8952\n",
      "Epoch 18/20\n",
      "149/149 [==============================] - 158s 1s/step - loss: 0.3064 - accuracy: 0.8992\n",
      "Epoch 19/20\n",
      "149/149 [==============================] - 153s 1s/step - loss: 0.2709 - accuracy: 0.9067\n",
      "Epoch 20/20\n",
      "149/149 [==============================] - 157s 1s/step - loss: 0.2846 - accuracy: 0.9072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f73e0338e50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images,batch_size=32,epochs=20,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a9d6eaf-d91f-4081-b33c-bd92bad5751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the test images to be classified with the help of flow_from_directory method from ImageDataGenerator class\n",
    "test_image_gen = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103911e7-32c8-40dc-8ad6-e132e9c09272",
   "metadata": {},
   "source": [
    "## Never forget to maintain all the test images under a single sub-directory in a directory because Keras treats a sub-folder as a separate class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "410f6468-42d5-42f5-a285-6cea20fd7bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 794 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# We are mentioning the same target size as before.\n",
    "# Coming to 'shuffle' parameter, I don't want the order of test images to be changed so I set it to False.\n",
    "# Mentioning 'batch_size' parameter as 1 because I want Keras to go through one test image for every moment. You can\n",
    "# mention 32 if you want Keras to consider 32 images at a time.\n",
    "test_images = test_image_gen.flow_from_directory('./test_images',target_size=(299,299),shuffle=False,batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc006d7c-e595-40a0-a856-9334a8c955ca",
   "metadata": {},
   "source": [
    "## Training done. Predict the test images with the help of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6bbceaa5-1472-446a-a440-6a75fe9f5b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794/794 [==============================] - 29s 31ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2db6141-2920-4f37-bd67-63c59c18c5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(794, 12)\n"
     ]
    }
   ],
   "source": [
    "# There should be 794 rows representing 794 images and 12 columns representing 12 classes of plant species.\n",
    "print(predictions.shape)  # Woohoo!! Correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5864daea-a714-46a1-b812-3de76dd2ea4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.8512020e-05, 3.8338217e-09, 2.4022557e-07, ..., 2.0378469e-05,\n",
       "        3.0488253e-04, 3.0773869e-08],\n",
       "       [5.8171154e-06, 9.3529451e-08, 3.1227860e-06, ..., 1.4535377e-03,\n",
       "        1.8914023e-05, 2.0412527e-07],\n",
       "       [5.7633162e-07, 9.6632086e-12, 2.6107316e-09, ..., 2.1695488e-04,\n",
       "        1.4394147e-03, 5.8251709e-11],\n",
       "       ...,\n",
       "       [2.3282331e-05, 1.4414688e-08, 3.7846653e-06, ..., 2.1360086e-04,\n",
       "        1.3802096e-03, 6.1799554e-08],\n",
       "       [4.9882534e-09, 4.4837519e-11, 1.1932534e-08, ..., 8.1049868e-05,\n",
       "        1.0026866e-02, 8.8916358e-10],\n",
       "       [3.8957673e-06, 7.5010540e-09, 3.2410921e-05, ..., 8.6403816e-05,\n",
       "        1.5669788e-05, 2.4188955e-08]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictions show the probability of every image(row) belonging to that particular class(column) among the 12.\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2f5d28d-cc2d-4fe1-a436-b0a8e86859b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So we have to identify index of the highest probability in every row implies that particular image is having \n",
    "# higher chances to belong to that plant species(column). We have argmax to do that.\n",
    "predictions[0].argmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9438a80e-5527-4b97-b0d2-f52af65b723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = [\"Black-grass\", \"Charlock\", \"Cleavers\", \"Common Chickweed\", \"Common wheat\", \"Fat Hen\",\n",
    "                \"Loose Silky-bent\", \"Maize\", \"Scentless Mayweed\", \"Shepherds Purse\", \"Small-flowered Cranesbill\",\n",
    "                \"Sugar beet\"]\n",
    "\n",
    "class_list = []\n",
    "\n",
    "# Identifying highest probability for all images and assigning it to the class it belongs.\n",
    "for i in range(0, 794):\n",
    "    y_class = predictions[i, :].argmax(axis=0)\n",
    "    class_list += [species_list[y_class]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8be9d4bb-49ae-4292-8f8c-4bafc29d68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Pandas Dataframe and creating required features.\n",
    "submission = pd.DataFrame()\n",
    "submission['file'] = test_images.filenames\n",
    "submission['file'] = submission['file'].str.replace('test/', '')\n",
    "submission['species'] = class_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c733039d-6d70-4ab3-b1f5-d4e1e1135841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file generated. All done.\n"
     ]
    }
   ],
   "source": [
    "# Converting the dataframe into csv_file for storing it permanently.\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('Submission file generated. All done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62f5a99-00dc-451d-92c7-865dbeec561a",
   "metadata": {},
   "source": [
    "## You can evaluate the predictions with the true values if you have true values already!! Predict function will do the task  - Finally Thank You"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
